services:
  nemotron:
    image: vllm/vllm-openai:latest
    container_name: nemotron-server
    ports:
      - "8081:8000"  # vLLM uses 8000 internally, expose on 8081
    volumes:
      - nemotron-cache:/root/.cache/huggingface  # Cache model downloads
    environment:
      - HF_HOME=/root/.cache/huggingface
      - VLLM_ATTENTION_BACKEND=FLASH_ATTN
    command: >
      --model nvidia/NVIDIA-Nemotron-Nano-9B-v2
      --trust-remote-code
      --dtype float16
      --max-model-len 2048
      --gpu-memory-utilization 0.95
      --quantization bitsandbytes
      --load-format bitsandbytes
      --enforce-eager
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 30s
      start_period: 900s  # 15 minutes for model download + load
      retries: 3

volumes:
  nemotron-cache:
    driver: local
